# 第三周工作总结
---------------------- 
## 1、实现KNN（k-nearest neighbors algorithm）
1、k邻近算法的输入为实例的特征向量，对对应于特征空间中的点；输出为实例的类别，可以取多类，k近邻法是建设给定一个训练数据集，其中的实例类别已定，分类时，对于新的实例，根据其k个最邻近的训练实例的类别，通过多数表决等方式进行预测。所以可以说，k近邻法不具有显示的学习过程。k临近算法实际上是利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”  
k值的选择，距离的度量和分类决策规则是k近邻算法的三个基本要素。   
[KNN算法实现](https://blog.csdn.net/qq_34784753/article/details/61917999)  
2、训练集
采用Cifar-10作为训练集  
[CIFAR-10数据集](https://www.cs.toronto.edu/~kriz/cifar.html)  
3、基于numpy库实现  
## 2、实现Logic回归
Logistic回归的主要思想是，根据现有的数据对分类边界建立回归公式，从而实现分类（一般两类）。“回归”的意思就是要找到最佳拟合参数，其中涉及的数学原理和步骤如下：   
（1）需要一个合适的分类函数来实现分类，单位阶跃函数、Sigmoid函数  
（2）损失函数（Cost函数）来表示预测值（h(x)h(x)）与实际值(yy)的偏差(h−yh−y),要使得回归最佳拟合，那么偏差要尽可能小（偏差求和或取均值）。  
（3）记J(ω)J(ω)表示回归系数为ωω时的偏差，那么求最佳回归参数ωω就转换成了求J(ω)J(ω)的最小值。 
详见：   
[Logic回归](https://blog.csdn.net/moxigandashu/article/details/72779856)

3、使用的数据集
使用的是csv格式的社会样本测试集，包括姓名，性别，年龄，体重，进行回归预测  
具体可在我的github上下载  
[social数据集](https://github.com/Ronanisbest/Task-note) 
